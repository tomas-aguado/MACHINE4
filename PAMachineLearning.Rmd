##Prediction Assignment Writeup

##Executive summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

In this assignment we will apply machine learning algorithm to the 20 test cases available in the test data above and submit out predictions. The following steps were followed in order to solve the tasks os the assignment:
* "Data loading, cleaning and preparation."
* "Data partition"
* "First model: Random forest"
* "Second model: Decision tree"
* "Conclusions and quiz generation"


##Data loading, cleaning and preparation
First of all we weill download the test and traing data and perform some basics data preparation tasks

```{r, results='markup', warning=F, message=F}
library(caret)
library(rpart)
library(randomForest)
library(rpart.plot)
library(MASS)
```

```{r, results='markup', warning=T, message=T}
#We set the global seed in order for our results to be reproducible.
set.seed(123456789)


trainurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# The we load the datasets putting NA values where needed.
training <- read.csv(trainurl, na.strings=c("NA","#DIV/0!", ""))
testing <- read.csv(testurl, na.strings=c("NA","#DIV/0!", ""))
```

Once loaded, leet's have a look at out datasets and see if we need to perform further transformations. 

```{r, results='hide', warning=T, message=T}
dim(training)
dim(testing)
```

```{r, results='markup', warning=F, message=F}
#We get rid of the columns containing NA values and useless variables
training<- training[, colSums(is.na(training)) == 0] 
testing <- testing[, colSums(is.na(testing)) == 0] 
training   <-training[,-c(1:7)]
testing <-testing[,-c(1:7)]
```




In order to perform cross validation we partition the data (75% -- 25%):
```{r, results='markup', warning=T, message=T}
dataPartition <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
Training_partition <- training[dataPartition, ] 
Testing_partition <- training[-dataPartition, ]
```

Once partitioned let's have a look at out subset data, in particular we should have a look at the classe variable distribution along the diferent possible levels:
```{r, results='markup', warning=T, message=T}
plot(Training_partition$classe, col="red", main="Classse distribution within partitioned training data", xlab="classe", ylab="Frequency")
```
##First model: Random Forest.
```{r, results='markup', warning=T, message=T}
mod1 <- randomForest(classe ~. , data=Training_partition, method="class")
pred1 <- predict(mod1, Testing_partition, type = "class")
confusionMatrix(pred1, Testing_partition$classe)
```
Altough random forest gave us outstanding values, we could check other models to see which precision thay can gave us

##Second mode: Decision tree
```{r, results='markup', warning=T, message=T}
mod2 <- rpart(classe ~ ., data=Training_partition, method="class")
pred2 <- predict(mod2, Testing_partition,type = "class")
confusionMatrix(pred2, Testing_partition$classe)

```
Seems like random forest is the best model that applies here, only 0.5% of out-of-sample error is a great value, it doesn't make sense to apply any other models like LDA. Now we will apply the random forest model to the test data, predict the values and answer the quiz.
```{r, results='markup', warning=T, message=T}
prediction <- predict(mod1, testing)
prediction
```
```